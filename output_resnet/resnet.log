Dataset ImageFolder
    Number of datapoints: 13594
    Root location: /home/kikuchio/courses/dl/project2/samples/train
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
Dataset ImageFolder
    Number of datapoints: 1697
    Root location: /home/kikuchio/courses/dl/project2/samples/validation
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
class names:  ['0', '1']
_Model(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 0/99
----------
train Loss: 0.3001 Acc: 0.8730
validation Loss: 0.2574 Acc: 0.8874

Epoch 1/99
----------
train Loss: 0.2631 Acc: 0.8880
validation Loss: 0.3627 Acc: 0.8185

Epoch 2/99
----------
train Loss: 0.2535 Acc: 0.8941
validation Loss: 0.2524 Acc: 0.8892

Epoch 3/99
----------
train Loss: 0.2468 Acc: 0.8969
validation Loss: 0.3405 Acc: 0.8816

Epoch 4/99
----------
train Loss: 0.2357 Acc: 0.9008
validation Loss: 0.2387 Acc: 0.9016

Epoch 5/99
----------
train Loss: 0.2240 Acc: 0.9073
validation Loss: 0.2783 Acc: 0.8898

Epoch 6/99
----------
train Loss: 0.2189 Acc: 0.9089
validation Loss: 0.2896 Acc: 0.8816

Epoch 7/99
----------
train Loss: 0.1853 Acc: 0.9229
validation Loss: 0.2144 Acc: 0.9087

Epoch 8/99
----------
train Loss: 0.1697 Acc: 0.9311
validation Loss: 0.2231 Acc: 0.9087

Epoch 9/99
----------
train Loss: 0.1633 Acc: 0.9320
validation Loss: 0.2304 Acc: 0.9063

Epoch 10/99
----------
train Loss: 0.1596 Acc: 0.9350
validation Loss: 0.2343 Acc: 0.9022

Epoch 11/99
----------
train Loss: 0.1538 Acc: 0.9375
validation Loss: 0.2285 Acc: 0.9010

Epoch 12/99
----------
train Loss: 0.1477 Acc: 0.9409
validation Loss: 0.2391 Acc: 0.9016

Epoch 13/99
----------
train Loss: 0.1448 Acc: 0.9429
validation Loss: 0.2311 Acc: 0.9063

Epoch 14/99
----------
train Loss: 0.1349 Acc: 0.9460
validation Loss: 0.2220 Acc: 0.9110

Epoch 15/99
----------
train Loss: 0.1291 Acc: 0.9495
validation Loss: 0.2248 Acc: 0.9063

Epoch 16/99
----------
train Loss: 0.1334 Acc: 0.9482
validation Loss: 0.2421 Acc: 0.9028

Epoch 17/99
----------
train Loss: 0.1315 Acc: 0.9484
validation Loss: 0.2406 Acc: 0.9016

Epoch 18/99
----------
train Loss: 0.1292 Acc: 0.9497
validation Loss: 0.2492 Acc: 0.9034

Epoch 19/99
----------
train Loss: 0.1302 Acc: 0.9490
validation Loss: 0.2541 Acc: 0.9034

Epoch 20/99
----------
train Loss: 0.1272 Acc: 0.9482
validation Loss: 0.2533 Acc: 0.8992

Epoch 21/99
----------
train Loss: 0.1271 Acc: 0.9498
validation Loss: 0.2382 Acc: 0.9087

Epoch 22/99
----------
train Loss: 0.1278 Acc: 0.9515
validation Loss: 0.2580 Acc: 0.9022

Epoch 23/99
----------
train Loss: 0.1279 Acc: 0.9497
validation Loss: 0.2548 Acc: 0.9028

Epoch 24/99
----------
train Loss: 0.1264 Acc: 0.9489
validation Loss: 0.2494 Acc: 0.9016

Epoch 25/99
----------
train Loss: 0.1297 Acc: 0.9477
validation Loss: 0.2563 Acc: 0.9016

Epoch 26/99
----------
train Loss: 0.1243 Acc: 0.9500
validation Loss: 0.2503 Acc: 0.9039

Epoch 27/99
----------
train Loss: 0.1279 Acc: 0.9506
validation Loss: 0.2460 Acc: 0.9081

Epoch 28/99
----------
train Loss: 0.1299 Acc: 0.9479
validation Loss: 0.2501 Acc: 0.9075

Epoch 29/99
----------
train Loss: 0.1293 Acc: 0.9470
validation Loss: 0.2426 Acc: 0.9028

Epoch 30/99
----------
train Loss: 0.1258 Acc: 0.9501
validation Loss: 0.2616 Acc: 0.9016

Epoch 31/99
----------
train Loss: 0.1257 Acc: 0.9508
validation Loss: 0.2524 Acc: 0.9004

Epoch 32/99
----------
train Loss: 0.1246 Acc: 0.9503
validation Loss: 0.2350 Acc: 0.9010

Epoch 33/99
----------
train Loss: 0.1274 Acc: 0.9502
validation Loss: 0.2493 Acc: 0.9004

Epoch 34/99
----------
train Loss: 0.1282 Acc: 0.9487
validation Loss: 0.2538 Acc: 0.9028

Epoch 35/99
----------
train Loss: 0.1245 Acc: 0.9509
validation Loss: 0.2551 Acc: 0.8992

Epoch 36/99
----------
train Loss: 0.1288 Acc: 0.9492
validation Loss: 0.2448 Acc: 0.9051

Epoch 37/99
----------
train Loss: 0.1250 Acc: 0.9503
validation Loss: 0.2502 Acc: 0.9022

Epoch 38/99
----------
train Loss: 0.1295 Acc: 0.9491
validation Loss: 0.2389 Acc: 0.9022

Epoch 39/99
----------
train Loss: 0.1260 Acc: 0.9516
validation Loss: 0.2485 Acc: 0.9045

Epoch 40/99
----------
train Loss: 0.1244 Acc: 0.9501
validation Loss: 0.2463 Acc: 0.9063

Epoch 41/99
----------
train Loss: 0.1259 Acc: 0.9496
validation Loss: 0.2442 Acc: 0.9039

Epoch 42/99
----------
train Loss: 0.1248 Acc: 0.9502
validation Loss: 0.2469 Acc: 0.9028

Epoch 43/99
----------
train Loss: 0.1272 Acc: 0.9503
validation Loss: 0.2483 Acc: 0.9016

Epoch 44/99
----------
train Loss: 0.1253 Acc: 0.9516
validation Loss: 0.2464 Acc: 0.9087

Epoch 45/99
----------
train Loss: 0.1287 Acc: 0.9491
validation Loss: 0.2517 Acc: 0.9057

Epoch 46/99
----------
train Loss: 0.1258 Acc: 0.9506
validation Loss: 0.2466 Acc: 0.9022

Epoch 47/99
----------
train Loss: 0.1234 Acc: 0.9502
validation Loss: 0.2465 Acc: 0.9010

Epoch 48/99
----------
train Loss: 0.1253 Acc: 0.9498
validation Loss: 0.2487 Acc: 0.9022

Epoch 49/99
----------
train Loss: 0.1281 Acc: 0.9492
validation Loss: 0.2473 Acc: 0.9081

Epoch 50/99
----------
train Loss: 0.1259 Acc: 0.9506
validation Loss: 0.2467 Acc: 0.9063

Epoch 51/99
----------
train Loss: 0.1219 Acc: 0.9528
validation Loss: 0.2443 Acc: 0.9104

Epoch 52/99
----------
train Loss: 0.1252 Acc: 0.9514
validation Loss: 0.2530 Acc: 0.9069

Epoch 53/99
----------
train Loss: 0.1287 Acc: 0.9491
validation Loss: 0.2509 Acc: 0.9034

Epoch 54/99
----------
train Loss: 0.1280 Acc: 0.9498
validation Loss: 0.2507 Acc: 0.9010

Epoch 55/99
----------
train Loss: 0.1241 Acc: 0.9517
validation Loss: 0.2423 Acc: 0.9075

Epoch 56/99
----------
train Loss: 0.1299 Acc: 0.9518
validation Loss: 0.2621 Acc: 0.8969

Epoch 57/99
----------
train Loss: 0.1287 Acc: 0.9487
validation Loss: 0.2391 Acc: 0.9063

Epoch 58/99
----------
train Loss: 0.1263 Acc: 0.9510
validation Loss: 0.2462 Acc: 0.9034

Epoch 59/99
----------
train Loss: 0.1222 Acc: 0.9526
validation Loss: 0.2516 Acc: 0.9022

Epoch 60/99
----------
train Loss: 0.1338 Acc: 0.9479
validation Loss: 0.2409 Acc: 0.9057

Epoch 61/99
----------
train Loss: 0.1249 Acc: 0.9488
validation Loss: 0.2511 Acc: 0.9028

Epoch 62/99
----------
train Loss: 0.1273 Acc: 0.9495
validation Loss: 0.2569 Acc: 0.9010

Epoch 63/99
----------
train Loss: 0.1247 Acc: 0.9505
validation Loss: 0.2532 Acc: 0.8969

Epoch 64/99
----------
train Loss: 0.1243 Acc: 0.9501
validation Loss: 0.2392 Acc: 0.9016

Epoch 65/99
----------
train Loss: 0.1279 Acc: 0.9502
validation Loss: 0.2502 Acc: 0.9016

Epoch 66/99
----------
train Loss: 0.1261 Acc: 0.9490
validation Loss: 0.2517 Acc: 0.9039

Epoch 67/99
----------
train Loss: 0.1214 Acc: 0.9523
validation Loss: 0.2434 Acc: 0.9004

Epoch 68/99
----------
train Loss: 0.1262 Acc: 0.9497
validation Loss: 0.2443 Acc: 0.9045

Epoch 69/99
----------
train Loss: 0.1257 Acc: 0.9490
validation Loss: 0.2532 Acc: 0.9034

Epoch 70/99
----------
train Loss: 0.1268 Acc: 0.9493
validation Loss: 0.2453 Acc: 0.9069

Epoch 71/99
----------
train Loss: 0.1261 Acc: 0.9495
validation Loss: 0.2614 Acc: 0.9022

Epoch 72/99
----------
train Loss: 0.1232 Acc: 0.9530
validation Loss: 0.2481 Acc: 0.9034

Epoch 73/99
----------
train Loss: 0.1252 Acc: 0.9503
validation Loss: 0.2578 Acc: 0.9057

Epoch 74/99
----------
train Loss: 0.1246 Acc: 0.9517
validation Loss: 0.2509 Acc: 0.8981

Epoch 75/99
----------
train Loss: 0.1273 Acc: 0.9481
validation Loss: 0.2467 Acc: 0.8986

Epoch 76/99
----------
train Loss: 0.1248 Acc: 0.9498
validation Loss: 0.2370 Acc: 0.9016

Epoch 77/99
----------
train Loss: 0.1280 Acc: 0.9473
validation Loss: 0.2543 Acc: 0.9039

Epoch 78/99
----------
train Loss: 0.1260 Acc: 0.9493
validation Loss: 0.2379 Acc: 0.9063

Epoch 79/99
----------
train Loss: 0.1264 Acc: 0.9490
validation Loss: 0.2463 Acc: 0.9028

Epoch 80/99
----------
train Loss: 0.1270 Acc: 0.9479
validation Loss: 0.2505 Acc: 0.9022

Epoch 81/99
----------
train Loss: 0.1266 Acc: 0.9501
validation Loss: 0.2541 Acc: 0.9016

Epoch 82/99
----------
train Loss: 0.1222 Acc: 0.9522
validation Loss: 0.2412 Acc: 0.9069

Epoch 83/99
----------
train Loss: 0.1228 Acc: 0.9519
validation Loss: 0.2675 Acc: 0.8998

Epoch 84/99
----------
train Loss: 0.1261 Acc: 0.9495
validation Loss: 0.2423 Acc: 0.9051

Epoch 85/99
----------
train Loss: 0.1273 Acc: 0.9510
validation Loss: 0.2447 Acc: 0.9057

Epoch 86/99
----------
train Loss: 0.1276 Acc: 0.9501
validation Loss: 0.2431 Acc: 0.9022

Epoch 87/99
----------
train Loss: 0.1278 Acc: 0.9503
validation Loss: 0.2420 Acc: 0.9051

Epoch 88/99
----------
train Loss: 0.1259 Acc: 0.9505
validation Loss: 0.2543 Acc: 0.9069

Epoch 89/99
----------
train Loss: 0.1283 Acc: 0.9501
validation Loss: 0.2568 Acc: 0.8957

Epoch 90/99
----------
train Loss: 0.1236 Acc: 0.9509
validation Loss: 0.2429 Acc: 0.8992

Epoch 91/99
----------
train Loss: 0.1262 Acc: 0.9506
validation Loss: 0.2467 Acc: 0.9028

Epoch 92/99
----------
train Loss: 0.1272 Acc: 0.9498
validation Loss: 0.2523 Acc: 0.9039

Epoch 93/99
----------
train Loss: 0.1268 Acc: 0.9494
validation Loss: 0.2463 Acc: 0.8998

Epoch 94/99
----------
train Loss: 0.1262 Acc: 0.9490
validation Loss: 0.2572 Acc: 0.9004

Epoch 95/99
----------
train Loss: 0.1212 Acc: 0.9514
validation Loss: 0.2517 Acc: 0.9016

Epoch 96/99
----------
train Loss: 0.1308 Acc: 0.9481
validation Loss: 0.2470 Acc: 0.9022

Epoch 97/99
----------
train Loss: 0.1240 Acc: 0.9506
validation Loss: 0.2484 Acc: 0.9010

Epoch 98/99
----------
train Loss: 0.1278 Acc: 0.9499
validation Loss: 0.2530 Acc: 0.9069

Epoch 99/99
----------
train Loss: 0.1252 Acc: 0.9516
validation Loss: 0.2495 Acc: 0.9051

Training complete in 996m 26s
Best validation Acc: 0.911019
test loss: 0.2413336899643023, test acc: 0.9068945197407189
Precision:  0.7096774193548387
Recall:  0.5
dataset size:  1697
sum:  1697
tp:  110
fp:  45
tn:  1432
fn:  110
F1 score:  0.5866666666666667
