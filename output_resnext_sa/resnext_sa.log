Dataset ImageFolder
    Number of datapoints: 13594
    Root location: /home/kikuchio/courses/dl/project2/samples/train
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
Dataset ImageFolder
    Number of datapoints: 1697
    Root location: /home/kikuchio/courses/dl/project2/samples/validation
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
class names:  ['0', '1']
_Model(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 0/99
----------
train Loss: 0.3025 Acc: 0.8748
validation Loss: 0.2632 Acc: 0.8845

Epoch 1/99
----------
train Loss: 0.2485 Acc: 0.8972
validation Loss: 0.2727 Acc: 0.8986

Epoch 2/99
----------
train Loss: 0.2313 Acc: 0.9018
validation Loss: 0.2667 Acc: 0.8986

Epoch 3/99
----------
train Loss: 0.2182 Acc: 0.9075
validation Loss: 0.2923 Acc: 0.8933

Epoch 4/99
----------
train Loss: 0.2046 Acc: 0.9156
validation Loss: 0.3536 Acc: 0.8945

Epoch 5/99
----------
train Loss: 0.1938 Acc: 0.9208
validation Loss: 0.2133 Acc: 0.9104

Epoch 6/99
----------
train Loss: 0.1802 Acc: 0.9281
validation Loss: 0.2215 Acc: 0.9057

Epoch 7/99
----------
train Loss: 0.1396 Acc: 0.9457
validation Loss: 0.2150 Acc: 0.9169

Epoch 8/99
----------
train Loss: 0.1224 Acc: 0.9506
validation Loss: 0.2244 Acc: 0.9175

Epoch 9/99
----------
train Loss: 0.1075 Acc: 0.9595
validation Loss: 0.2242 Acc: 0.9175

Epoch 10/99
----------
train Loss: 0.1036 Acc: 0.9612
validation Loss: 0.2431 Acc: 0.9151

Epoch 11/99
----------
train Loss: 0.0951 Acc: 0.9626
validation Loss: 0.2443 Acc: 0.9104

Epoch 12/99
----------
train Loss: 0.0857 Acc: 0.9667
validation Loss: 0.2832 Acc: 0.9146

Epoch 13/99
----------
train Loss: 0.0772 Acc: 0.9716
validation Loss: 0.2831 Acc: 0.9181

Epoch 14/99
----------
train Loss: 0.0712 Acc: 0.9740
validation Loss: 0.2851 Acc: 0.9104

Epoch 15/99
----------
train Loss: 0.0678 Acc: 0.9757
validation Loss: 0.2890 Acc: 0.9057

Epoch 16/99
----------
train Loss: 0.0649 Acc: 0.9759
validation Loss: 0.2787 Acc: 0.9146

Epoch 17/99
----------
train Loss: 0.0685 Acc: 0.9760
validation Loss: 0.2946 Acc: 0.9122

Epoch 18/99
----------
train Loss: 0.0650 Acc: 0.9769
validation Loss: 0.3102 Acc: 0.9116

Epoch 19/99
----------
train Loss: 0.0577 Acc: 0.9803
validation Loss: 0.2976 Acc: 0.9175

Epoch 20/99
----------
train Loss: 0.0622 Acc: 0.9767
validation Loss: 0.3025 Acc: 0.9116

Epoch 21/99
----------
train Loss: 0.0586 Acc: 0.9773
validation Loss: 0.3050 Acc: 0.9116

Epoch 22/99
----------
train Loss: 0.0595 Acc: 0.9793
validation Loss: 0.2907 Acc: 0.9128

Epoch 23/99
----------
train Loss: 0.0613 Acc: 0.9789
validation Loss: 0.2953 Acc: 0.9151

Epoch 24/99
----------
train Loss: 0.0559 Acc: 0.9807
validation Loss: 0.2872 Acc: 0.9169

Epoch 25/99
----------
train Loss: 0.0636 Acc: 0.9776
validation Loss: 0.3015 Acc: 0.9146

Epoch 26/99
----------
train Loss: 0.0608 Acc: 0.9773
validation Loss: 0.2865 Acc: 0.9169

Epoch 27/99
----------
train Loss: 0.0572 Acc: 0.9794
validation Loss: 0.2942 Acc: 0.9128

Epoch 28/99
----------
train Loss: 0.0610 Acc: 0.9784
validation Loss: 0.2844 Acc: 0.9116

Epoch 29/99
----------
train Loss: 0.0575 Acc: 0.9803
validation Loss: 0.3028 Acc: 0.9087

Epoch 30/99
----------
train Loss: 0.0634 Acc: 0.9759
validation Loss: 0.2957 Acc: 0.9157

Epoch 31/99
----------
train Loss: 0.0556 Acc: 0.9804
validation Loss: 0.3067 Acc: 0.9151

Epoch 32/99
----------
train Loss: 0.0578 Acc: 0.9794
validation Loss: 0.2843 Acc: 0.9151

Epoch 33/99
----------
train Loss: 0.0592 Acc: 0.9795
validation Loss: 0.2990 Acc: 0.9128

Epoch 34/99
----------
train Loss: 0.0636 Acc: 0.9782
validation Loss: 0.2965 Acc: 0.9146

Epoch 35/99
----------
train Loss: 0.0597 Acc: 0.9784
validation Loss: 0.2899 Acc: 0.9140

Epoch 36/99
----------
train Loss: 0.0569 Acc: 0.9790
validation Loss: 0.2951 Acc: 0.9169

Epoch 37/99
----------
train Loss: 0.0590 Acc: 0.9793
validation Loss: 0.2869 Acc: 0.9187

Epoch 38/99
----------
train Loss: 0.0546 Acc: 0.9809
validation Loss: 0.2862 Acc: 0.9181

Epoch 39/99
----------
train Loss: 0.0621 Acc: 0.9769
validation Loss: 0.2940 Acc: 0.9069

Epoch 40/99
----------
train Loss: 0.1426 Acc: 0.9456
validation Loss: 0.3268 Acc: 0.8969

Epoch 41/99
----------
train Loss: 0.1328 Acc: 0.9489
validation Loss: 0.3028 Acc: 0.9010

Epoch 42/99
----------
train Loss: 0.1267 Acc: 0.9498
validation Loss: 0.2600 Acc: 0.9069

Epoch 43/99
----------
train Loss: 0.1212 Acc: 0.9548
validation Loss: 0.2930 Acc: 0.8957

Epoch 44/99
----------
train Loss: 0.1134 Acc: 0.9556
validation Loss: 0.2402 Acc: 0.9110

Epoch 45/99
----------
train Loss: 0.1026 Acc: 0.9609
validation Loss: 0.3232 Acc: 0.8992

Epoch 46/99
----------
train Loss: 0.1005 Acc: 0.9619
validation Loss: 0.3899 Acc: 0.8975

Epoch 47/99
----------
train Loss: 0.0643 Acc: 0.9789
validation Loss: 0.2702 Acc: 0.9134

Epoch 48/99
----------
train Loss: 0.0427 Acc: 0.9840
validation Loss: 0.3165 Acc: 0.9116

Epoch 49/99
----------
train Loss: 0.0396 Acc: 0.9868
validation Loss: 0.3074 Acc: 0.9093

Epoch 50/99
----------
train Loss: 0.0323 Acc: 0.9893
validation Loss: 0.3468 Acc: 0.9116

Epoch 51/99
----------
train Loss: 0.0261 Acc: 0.9913
validation Loss: 0.3880 Acc: 0.9075

Epoch 52/99
----------
train Loss: 0.0268 Acc: 0.9910
validation Loss: 0.4034 Acc: 0.9175

Epoch 53/99
----------
train Loss: 0.0182 Acc: 0.9942
validation Loss: 0.3924 Acc: 0.9146

Epoch 54/99
----------
train Loss: 0.0146 Acc: 0.9954
validation Loss: 0.4006 Acc: 0.9140

Epoch 55/99
----------
train Loss: 0.0150 Acc: 0.9957
validation Loss: 0.4766 Acc: 0.9081

Epoch 56/99
----------
train Loss: 0.0118 Acc: 0.9969
validation Loss: 0.3778 Acc: 0.9128

Epoch 57/99
----------
train Loss: 0.0125 Acc: 0.9965
validation Loss: 0.4299 Acc: 0.9140

Epoch 58/99
----------
train Loss: 0.0119 Acc: 0.9966
validation Loss: 0.4320 Acc: 0.9134

Epoch 59/99
----------
train Loss: 0.0139 Acc: 0.9954
validation Loss: 0.4318 Acc: 0.9163

Epoch 60/99
----------
train Loss: 0.0116 Acc: 0.9961
validation Loss: 0.4438 Acc: 0.9134

Epoch 61/99
----------
train Loss: 0.0120 Acc: 0.9969
validation Loss: 0.4364 Acc: 0.9081

Epoch 62/99
----------
train Loss: 0.0129 Acc: 0.9963
validation Loss: 0.4430 Acc: 0.9140

Epoch 63/99
----------
train Loss: 0.0088 Acc: 0.9974
validation Loss: 0.4090 Acc: 0.9187

Epoch 64/99
----------
train Loss: 0.0097 Acc: 0.9972
validation Loss: 0.4381 Acc: 0.9098

Epoch 65/99
----------
train Loss: 0.0114 Acc: 0.9965
validation Loss: 0.4314 Acc: 0.9140

Epoch 66/99
----------
train Loss: 0.0129 Acc: 0.9965
validation Loss: 0.4349 Acc: 0.9146

Epoch 67/99
----------
train Loss: 0.0117 Acc: 0.9963
validation Loss: 0.4321 Acc: 0.9222

Epoch 68/99
----------
train Loss: 0.0096 Acc: 0.9971
validation Loss: 0.4161 Acc: 0.9140

Epoch 69/99
----------
train Loss: 0.0114 Acc: 0.9967
validation Loss: 0.4524 Acc: 0.9116

Epoch 70/99
----------
train Loss: 0.0116 Acc: 0.9962
validation Loss: 0.4470 Acc: 0.9140

Epoch 71/99
----------
train Loss: 0.0110 Acc: 0.9968
validation Loss: 0.4419 Acc: 0.9146

Epoch 72/99
----------
train Loss: 0.0132 Acc: 0.9963
validation Loss: 0.4563 Acc: 0.9093

Epoch 73/99
----------
train Loss: 0.0098 Acc: 0.9974
validation Loss: 0.4261 Acc: 0.9163

Epoch 74/99
----------
train Loss: 0.0102 Acc: 0.9965
validation Loss: 0.4414 Acc: 0.9169

Epoch 75/99
----------
train Loss: 0.0120 Acc: 0.9962
validation Loss: 0.4454 Acc: 0.9116

Epoch 76/99
----------
train Loss: 0.0111 Acc: 0.9968
validation Loss: 0.4095 Acc: 0.9193

Epoch 77/99
----------
train Loss: 0.0106 Acc: 0.9970
validation Loss: 0.4340 Acc: 0.9104

Epoch 78/99
----------
train Loss: 0.0117 Acc: 0.9967
validation Loss: 0.4451 Acc: 0.9098

Epoch 79/99
----------
train Loss: 0.0127 Acc: 0.9963
validation Loss: 0.4484 Acc: 0.9057

Epoch 80/99
----------
train Loss: 0.0118 Acc: 0.9967
validation Loss: 0.4339 Acc: 0.9098

Epoch 81/99
----------
train Loss: 0.0116 Acc: 0.9967
validation Loss: 0.4077 Acc: 0.9140

Epoch 82/99
----------
train Loss: 0.0094 Acc: 0.9974
validation Loss: 0.4309 Acc: 0.9146

Epoch 83/99
----------
train Loss: 0.0101 Acc: 0.9974
validation Loss: 0.4518 Acc: 0.9075

Epoch 84/99
----------
train Loss: 0.0119 Acc: 0.9974
validation Loss: 0.4457 Acc: 0.9051

Epoch 85/99
----------
train Loss: 0.0112 Acc: 0.9962
validation Loss: 0.4196 Acc: 0.9157

Epoch 86/99
----------
train Loss: 0.0099 Acc: 0.9969
validation Loss: 0.4612 Acc: 0.9104

Epoch 87/99
----------
train Loss: 0.0114 Acc: 0.9971
validation Loss: 0.4248 Acc: 0.9146

Epoch 88/99
----------
train Loss: 0.0116 Acc: 0.9962
validation Loss: 0.4525 Acc: 0.9075

Epoch 89/99
----------
train Loss: 0.0121 Acc: 0.9961
validation Loss: 0.4397 Acc: 0.9151

Epoch 90/99
----------
train Loss: 0.0089 Acc: 0.9977
validation Loss: 0.4718 Acc: 0.9104

Epoch 91/99
----------
train Loss: 0.0106 Acc: 0.9968
validation Loss: 0.4498 Acc: 0.9151

Epoch 92/99
----------
train Loss: 0.0096 Acc: 0.9976
validation Loss: 0.4343 Acc: 0.9169

Epoch 93/99
----------
train Loss: 0.0096 Acc: 0.9972
validation Loss: 0.4391 Acc: 0.9128

Epoch 94/99
----------
train Loss: 0.0115 Acc: 0.9959
validation Loss: 0.4184 Acc: 0.9146

Epoch 95/99
----------
train Loss: 0.0110 Acc: 0.9969
validation Loss: 0.4086 Acc: 0.9151

Epoch 96/99
----------
train Loss: 0.0092 Acc: 0.9972
validation Loss: 0.4383 Acc: 0.9134

Epoch 97/99
----------
train Loss: 0.0118 Acc: 0.9960
validation Loss: 0.4371 Acc: 0.9175

Epoch 98/99
----------
train Loss: 0.0106 Acc: 0.9968
validation Loss: 0.4287 Acc: 0.9181

Training complete in 574m 28s
Best validation Acc: 0.922216
test loss: 0.4250016520672429, test acc: 0.9104301708898055
