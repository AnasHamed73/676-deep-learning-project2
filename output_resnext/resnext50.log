Dataset ImageFolder
    Number of datapoints: 9764
    Root location: /home/kikuchio/courses/dl/project2/samples/train
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
Dataset ImageFolder
    Number of datapoints: 1221
    Root location: /home/kikuchio/courses/dl/project2/samples/validation
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
class names:  ['0', '1']
_Model(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)
        (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 0/99
----------
train Loss: 0.3997 Acc: 0.8296
validation Loss: 0.6492 Acc: 0.8485

Epoch 1/99
----------
train Loss: 0.3377 Acc: 0.8536
validation Loss: 0.3601 Acc: 0.8640

Epoch 2/99
----------
train Loss: 0.3093 Acc: 0.8670
validation Loss: 0.3708 Acc: 0.8370

Epoch 3/99
----------
train Loss: 0.2885 Acc: 0.8798
validation Loss: 0.2968 Acc: 0.8731

Epoch 4/99
----------
train Loss: 0.2709 Acc: 0.8864
validation Loss: 0.3354 Acc: 0.8608

Epoch 5/99
----------
train Loss: 0.2566 Acc: 0.8956
validation Loss: 0.3346 Acc: 0.8812

Epoch 6/99
----------
train Loss: 0.2390 Acc: 0.9032
validation Loss: 0.3187 Acc: 0.8583

Epoch 7/99
----------
train Loss: 0.1763 Acc: 0.9318
validation Loss: 0.2940 Acc: 0.8755

Epoch 8/99
----------
train Loss: 0.1486 Acc: 0.9462
validation Loss: 0.3271 Acc: 0.8739

Epoch 9/99
----------
train Loss: 0.1362 Acc: 0.9480
validation Loss: 0.3185 Acc: 0.8763

Epoch 10/99
----------
train Loss: 0.1302 Acc: 0.9528
validation Loss: 0.3213 Acc: 0.8821

Epoch 11/99
----------
train Loss: 0.1163 Acc: 0.9579
validation Loss: 0.3473 Acc: 0.8771

Epoch 12/99
----------
train Loss: 0.1090 Acc: 0.9615
validation Loss: 0.3296 Acc: 0.8886

Epoch 13/99
----------
train Loss: 0.1011 Acc: 0.9655
validation Loss: 0.3655 Acc: 0.8763

Epoch 14/99
----------
train Loss: 0.0835 Acc: 0.9719
validation Loss: 0.3577 Acc: 0.8886

Epoch 15/99
----------
train Loss: 0.0781 Acc: 0.9724
validation Loss: 0.3440 Acc: 0.8935

Epoch 16/99
----------
train Loss: 0.0851 Acc: 0.9713
validation Loss: 0.3538 Acc: 0.8919

Epoch 17/99
----------
train Loss: 0.0837 Acc: 0.9727
validation Loss: 0.3930 Acc: 0.8763

Epoch 18/99
----------
train Loss: 0.0755 Acc: 0.9748
validation Loss: 0.3813 Acc: 0.8886

Epoch 19/99
----------
train Loss: 0.0773 Acc: 0.9734
validation Loss: 0.3593 Acc: 0.8878

Epoch 20/99
----------
train Loss: 0.0792 Acc: 0.9733
validation Loss: 0.4103 Acc: 0.8763

Epoch 21/99
----------
train Loss: 0.0793 Acc: 0.9743
validation Loss: 0.3725 Acc: 0.8845

Epoch 22/99
----------
train Loss: 0.0727 Acc: 0.9763
validation Loss: 0.4012 Acc: 0.8821

Epoch 23/99
----------
train Loss: 0.0757 Acc: 0.9749
validation Loss: 0.3805 Acc: 0.8943

Epoch 24/99
----------
train Loss: 0.0760 Acc: 0.9738
validation Loss: 0.3620 Acc: 0.8894

Epoch 25/99
----------
train Loss: 0.0766 Acc: 0.9716
validation Loss: 0.3964 Acc: 0.8780

Epoch 26/99
----------
train Loss: 0.0776 Acc: 0.9722
validation Loss: 0.3843 Acc: 0.8821

Epoch 27/99
----------
train Loss: 0.0779 Acc: 0.9734
validation Loss: 0.3936 Acc: 0.8804

Epoch 28/99
----------
train Loss: 0.0731 Acc: 0.9758
validation Loss: 0.3759 Acc: 0.8812

Epoch 29/99
----------
train Loss: 0.0744 Acc: 0.9747
validation Loss: 0.3884 Acc: 0.8739

Epoch 30/99
----------
train Loss: 0.0683 Acc: 0.9757
validation Loss: 0.3865 Acc: 0.8837

Epoch 31/99
----------
train Loss: 0.0699 Acc: 0.9772
validation Loss: 0.3856 Acc: 0.8853

Epoch 32/99
----------
train Loss: 0.0688 Acc: 0.9761
validation Loss: 0.3730 Acc: 0.8903

Epoch 33/99
----------
train Loss: 0.0760 Acc: 0.9738
validation Loss: 0.4158 Acc: 0.8804

Epoch 34/99
----------
train Loss: 0.0714 Acc: 0.9773
validation Loss: 0.4002 Acc: 0.8780

Epoch 35/99
----------
train Loss: 0.0740 Acc: 0.9752
validation Loss: 0.3686 Acc: 0.8862

Epoch 36/99
----------
train Loss: 0.0677 Acc: 0.9771
validation Loss: 0.3828 Acc: 0.8878

Epoch 37/99
----------
train Loss: 0.0715 Acc: 0.9746
validation Loss: 0.3833 Acc: 0.8796

Epoch 38/99
----------
train Loss: 0.0729 Acc: 0.9762
validation Loss: 0.3876 Acc: 0.8812

Epoch 39/99
----------
train Loss: 0.0717 Acc: 0.9768
validation Loss: 0.4010 Acc: 0.8788

Epoch 40/99
----------
train Loss: 0.0713 Acc: 0.9775
validation Loss: 0.3981 Acc: 0.8845

Epoch 41/99
----------
train Loss: 0.0738 Acc: 0.9736
validation Loss: 0.3910 Acc: 0.8853

Epoch 42/99
----------
train Loss: 0.0740 Acc: 0.9750
validation Loss: 0.4029 Acc: 0.8812

Epoch 43/99
----------
train Loss: 0.0762 Acc: 0.9741
validation Loss: 0.3878 Acc: 0.8853

Epoch 44/99
----------
train Loss: 0.0754 Acc: 0.9752
validation Loss: 0.4102 Acc: 0.8829

Epoch 45/99
----------
train Loss: 0.0807 Acc: 0.9737
validation Loss: 0.3784 Acc: 0.8919

Epoch 46/99
----------
train Loss: 0.0776 Acc: 0.9739
validation Loss: 0.4061 Acc: 0.8886

Epoch 47/99
----------
train Loss: 0.0718 Acc: 0.9751
validation Loss: 0.3969 Acc: 0.8845

Epoch 48/99
----------
train Loss: 0.0725 Acc: 0.9735
validation Loss: 0.4025 Acc: 0.8845

Epoch 49/99
----------
train Loss: 0.0792 Acc: 0.9747
validation Loss: 0.3911 Acc: 0.8829

Epoch 50/99
----------
train Loss: 0.0764 Acc: 0.9737
validation Loss: 0.4140 Acc: 0.8771

Epoch 51/99
----------
train Loss: 0.0762 Acc: 0.9746
validation Loss: 0.4026 Acc: 0.8845

Epoch 52/99
----------
train Loss: 0.0680 Acc: 0.9763
validation Loss: 0.3703 Acc: 0.8903

Epoch 53/99
----------
train Loss: 0.0768 Acc: 0.9736
validation Loss: 0.3651 Acc: 0.8837

Epoch 54/99
----------
train Loss: 0.0715 Acc: 0.9757
validation Loss: 0.3972 Acc: 0.8829

Epoch 55/99
----------
train Loss: 0.0711 Acc: 0.9761
validation Loss: 0.3848 Acc: 0.8894

Epoch 56/99
----------
train Loss: 0.0728 Acc: 0.9745
validation Loss: 0.3755 Acc: 0.8845

Epoch 57/99
----------
train Loss: 0.0756 Acc: 0.9757
validation Loss: 0.4119 Acc: 0.8796

Epoch 58/99
----------
train Loss: 0.0742 Acc: 0.9744
validation Loss: 0.3722 Acc: 0.8903

Epoch 59/99
----------
train Loss: 0.0755 Acc: 0.9745
validation Loss: 0.4022 Acc: 0.8870

Epoch 60/99
----------
train Loss: 0.0734 Acc: 0.9765
validation Loss: 0.3921 Acc: 0.8812

Epoch 61/99
----------
train Loss: 0.0739 Acc: 0.9741
validation Loss: 0.3660 Acc: 0.8886

Epoch 62/99
----------
train Loss: 0.0721 Acc: 0.9762
validation Loss: 0.3750 Acc: 0.8853

Epoch 63/99
----------
train Loss: 0.0727 Acc: 0.9756
validation Loss: 0.3592 Acc: 0.8911

Epoch 64/99
----------
train Loss: 0.0708 Acc: 0.9763
validation Loss: 0.3823 Acc: 0.8886

Epoch 65/99
----------
train Loss: 0.0713 Acc: 0.9749
validation Loss: 0.3789 Acc: 0.8845

Epoch 66/99
----------
train Loss: 0.0761 Acc: 0.9733
validation Loss: 0.4048 Acc: 0.8722

Epoch 67/99
----------
train Loss: 0.0752 Acc: 0.9761
validation Loss: 0.4111 Acc: 0.8796

Epoch 68/99
----------
train Loss: 0.0750 Acc: 0.9745
validation Loss: 0.3739 Acc: 0.8886

Epoch 69/99
----------
train Loss: 0.0741 Acc: 0.9729
validation Loss: 0.3978 Acc: 0.8804

Epoch 70/99
----------
train Loss: 0.0707 Acc: 0.9760
validation Loss: 0.3764 Acc: 0.8804

Epoch 71/99
----------
train Loss: 0.0727 Acc: 0.9755
validation Loss: 0.3632 Acc: 0.8894

Epoch 72/99
----------
train Loss: 0.0717 Acc: 0.9758
validation Loss: 0.3837 Acc: 0.8878

Epoch 73/99
----------
train Loss: 0.0818 Acc: 0.9720
validation Loss: 0.3931 Acc: 0.8845

Epoch 74/99
----------
train Loss: 0.0757 Acc: 0.9735
validation Loss: 0.3967 Acc: 0.8788

Epoch 75/99
----------
train Loss: 0.0719 Acc: 0.9755
validation Loss: 0.4027 Acc: 0.8763

Epoch 76/99
----------
train Loss: 0.0731 Acc: 0.9759
validation Loss: 0.3829 Acc: 0.8821

Epoch 77/99
----------
train Loss: 0.0716 Acc: 0.9764
validation Loss: 0.4197 Acc: 0.8837

Epoch 78/99
----------
train Loss: 0.0738 Acc: 0.9739
validation Loss: 0.3578 Acc: 0.8927

Epoch 79/99
----------
train Loss: 0.0752 Acc: 0.9728
validation Loss: 0.3710 Acc: 0.8919

Epoch 80/99
----------
train Loss: 0.0756 Acc: 0.9747
validation Loss: 0.4114 Acc: 0.8804

Epoch 81/99
----------
train Loss: 0.0767 Acc: 0.9744
validation Loss: 0.3995 Acc: 0.8763

Epoch 82/99
----------
train Loss: 0.0677 Acc: 0.9773
validation Loss: 0.3886 Acc: 0.8870

Epoch 83/99
----------
train Loss: 0.0762 Acc: 0.9741
validation Loss: 0.3747 Acc: 0.8862

Epoch 84/99
----------
train Loss: 0.0738 Acc: 0.9770
validation Loss: 0.3953 Acc: 0.8870

Epoch 85/99
----------
train Loss: 0.0720 Acc: 0.9749
validation Loss: 0.3960 Acc: 0.8886

Epoch 86/99
----------
train Loss: 0.0732 Acc: 0.9761
validation Loss: 0.3856 Acc: 0.8886

Epoch 87/99
----------
train Loss: 0.0724 Acc: 0.9749
validation Loss: 0.4111 Acc: 0.8804

Epoch 88/99
----------
train Loss: 0.0699 Acc: 0.9752
validation Loss: 0.3814 Acc: 0.8862

Epoch 89/99
----------
train Loss: 0.0724 Acc: 0.9752
validation Loss: 0.3755 Acc: 0.8903

Epoch 90/99
----------
train Loss: 0.0767 Acc: 0.9736
validation Loss: 0.3678 Acc: 0.8812

Epoch 91/99
----------
train Loss: 0.0753 Acc: 0.9743
validation Loss: 0.3890 Acc: 0.8911

Epoch 92/99
----------
train Loss: 0.0736 Acc: 0.9753
validation Loss: 0.3906 Acc: 0.8788

Epoch 93/99
----------
train Loss: 0.0752 Acc: 0.9749
validation Loss: 0.3677 Acc: 0.8943

Epoch 94/99
----------
train Loss: 0.0752 Acc: 0.9756
validation Loss: 0.3983 Acc: 0.8812

Epoch 95/99
----------
train Loss: 0.0756 Acc: 0.9735
validation Loss: 0.4142 Acc: 0.8837

Epoch 96/99
----------
train Loss: 0.0696 Acc: 0.9776
validation Loss: 0.3791 Acc: 0.8821

Epoch 97/99
----------
train Loss: 0.0665 Acc: 0.9785
validation Loss: 0.4152 Acc: 0.8706

Epoch 98/99
----------
train Loss: 0.0757 Acc: 0.9751
validation Loss: 0.3755 Acc: 0.8853

Epoch 99/99
----------
train Loss: 0.0708 Acc: 0.9762
validation Loss: 0.4034 Acc: 0.8747

Training complete in 887m 24s
Best validation Acc: 0.894349
test loss: 0.4106368481568278, test acc: 0.8746928746928747
