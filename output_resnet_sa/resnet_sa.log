Dataset ImageFolder
    Number of datapoints: 9764
    Root location: /home/kikuchio/courses/dl/project2/samples/train
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
Dataset ImageFolder
    Number of datapoints: 1221
    Root location: /home/kikuchio/courses/dl/project2/samples/validation
    StandardTransform
Transform: Compose(
               Resize(size=(128, 128), interpolation=PIL.Image.BILINEAR)
               RandomHorizontalFlip(p=0.5)
               RandomVerticalFlip(p=0.5)
               ToTensor()
               Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))
           )
class names:  ['0', '1']
_Model(
  (model): ResNet(
    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (relu): ReLU(inplace=True)
    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
    (layer1): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer2): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer3): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (3): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (4): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (5): Bottleneck(
        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (layer4): Sequential(
      (0): Bottleneck(
        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
        (downsample): Sequential(
          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        )
      )
      (1): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
      (2): Bottleneck(
        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (relu): ReLU(inplace=True)
      )
    )
    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))
    (fc): Linear(in_features=2048, out_features=2, bias=True)
  )
)
Epoch 0/99
----------
train Loss: 0.4025 Acc: 0.8261
validation Loss: 0.4882 Acc: 0.7674

Epoch 1/99
----------
train Loss: 0.3491 Acc: 0.8436
validation Loss: 0.3605 Acc: 0.8296

Epoch 2/99
----------
train Loss: 0.3312 Acc: 0.8556
validation Loss: 0.3684 Acc: 0.8460

Epoch 3/99
----------
train Loss: 0.3227 Acc: 0.8607
validation Loss: 0.3029 Acc: 0.8690

Epoch 4/99
----------
train Loss: 0.3060 Acc: 0.8696
validation Loss: 0.3386 Acc: 0.8526

Epoch 5/99
----------
train Loss: 0.2989 Acc: 0.8712
validation Loss: 0.3225 Acc: 0.8542

Epoch 6/99
----------
train Loss: 0.2926 Acc: 0.8760
validation Loss: 0.3718 Acc: 0.8280

Epoch 7/99
----------
train Loss: 0.2415 Acc: 0.8992
validation Loss: 0.2849 Acc: 0.8698

Epoch 8/99
----------
train Loss: 0.2191 Acc: 0.9119
validation Loss: 0.2923 Acc: 0.8755

Epoch 9/99
----------
train Loss: 0.2092 Acc: 0.9179
validation Loss: 0.3071 Acc: 0.8608

Epoch 10/99
----------
train Loss: 0.1993 Acc: 0.9205
validation Loss: 0.2962 Acc: 0.8747

Epoch 11/99
----------
train Loss: 0.1919 Acc: 0.9242
validation Loss: 0.3033 Acc: 0.8747

Epoch 12/99
----------
train Loss: 0.1799 Acc: 0.9296
validation Loss: 0.3164 Acc: 0.8747

Epoch 13/99
----------
train Loss: 0.1716 Acc: 0.9344
validation Loss: 0.3156 Acc: 0.8731

Epoch 14/99
----------
train Loss: 0.1574 Acc: 0.9417
validation Loss: 0.3205 Acc: 0.8665

Epoch 15/99
----------
train Loss: 0.1611 Acc: 0.9401
validation Loss: 0.3164 Acc: 0.8665

Epoch 16/99
----------
train Loss: 0.1552 Acc: 0.9412
validation Loss: 0.3229 Acc: 0.8624

Epoch 17/99
----------
train Loss: 0.1519 Acc: 0.9411
validation Loss: 0.3359 Acc: 0.8649

Epoch 18/99
----------
train Loss: 0.1521 Acc: 0.9436
validation Loss: 0.3306 Acc: 0.8591

Epoch 19/99
----------
train Loss: 0.1494 Acc: 0.9461
validation Loss: 0.3284 Acc: 0.8706

Epoch 20/99
----------
train Loss: 0.1479 Acc: 0.9467
validation Loss: 0.3408 Acc: 0.8698

Epoch 21/99
----------
train Loss: 0.1520 Acc: 0.9420
validation Loss: 0.3278 Acc: 0.8747

Epoch 22/99
----------
train Loss: 0.1479 Acc: 0.9439
validation Loss: 0.3222 Acc: 0.8722

Epoch 23/99
----------
train Loss: 0.1467 Acc: 0.9475
validation Loss: 0.3336 Acc: 0.8657

Epoch 24/99
----------
train Loss: 0.1486 Acc: 0.9443
validation Loss: 0.3498 Acc: 0.8649

Epoch 25/99
----------
train Loss: 0.1406 Acc: 0.9464
validation Loss: 0.3499 Acc: 0.8698

Epoch 26/99
----------
train Loss: 0.1487 Acc: 0.9432
validation Loss: 0.3441 Acc: 0.8559

Epoch 27/99
----------
train Loss: 0.1468 Acc: 0.9443
validation Loss: 0.3625 Acc: 0.8559

Epoch 28/99
----------
train Loss: 0.1496 Acc: 0.9432
validation Loss: 0.3556 Acc: 0.8608

Epoch 29/99
----------
train Loss: 0.1469 Acc: 0.9461
validation Loss: 0.3435 Acc: 0.8706

Epoch 30/99
----------
train Loss: 0.1503 Acc: 0.9436
validation Loss: 0.3304 Acc: 0.8665

Epoch 31/99
----------
train Loss: 0.1438 Acc: 0.9486
validation Loss: 0.3556 Acc: 0.8624

Epoch 32/99
----------
train Loss: 0.1448 Acc: 0.9471
validation Loss: 0.3316 Acc: 0.8681

Epoch 33/99
----------
train Loss: 0.1503 Acc: 0.9435
validation Loss: 0.3582 Acc: 0.8640

Epoch 34/99
----------
train Loss: 0.1459 Acc: 0.9460
validation Loss: 0.3378 Acc: 0.8649

Epoch 35/99
----------
train Loss: 0.1480 Acc: 0.9463
validation Loss: 0.3566 Acc: 0.8673

Epoch 36/99
----------
train Loss: 0.1456 Acc: 0.9464
validation Loss: 0.3231 Acc: 0.8681

Epoch 37/99
----------
train Loss: 0.1495 Acc: 0.9432
validation Loss: 0.3336 Acc: 0.8665

Epoch 38/99
----------
train Loss: 0.1484 Acc: 0.9437
validation Loss: 0.3429 Acc: 0.8690

Epoch 39/99
----------
train Loss: 0.1524 Acc: 0.9436
validation Loss: 0.3523 Acc: 0.8640

Epoch 40/99
----------
train Loss: 0.1488 Acc: 0.9439
validation Loss: 0.3469 Acc: 0.8632

Epoch 41/99
----------
train Loss: 0.1441 Acc: 0.9475
validation Loss: 0.3126 Acc: 0.8698

Epoch 42/99
----------
train Loss: 0.1448 Acc: 0.9468
validation Loss: 0.3378 Acc: 0.8632

Epoch 43/99
----------
train Loss: 0.1521 Acc: 0.9417
validation Loss: 0.3340 Acc: 0.8739

Epoch 44/99
----------
train Loss: 0.1537 Acc: 0.9441
validation Loss: 0.3548 Acc: 0.8640

Epoch 45/99
----------
train Loss: 0.1471 Acc: 0.9457
validation Loss: 0.3328 Acc: 0.8640

Epoch 46/99
----------
train Loss: 0.1484 Acc: 0.9443
validation Loss: 0.3595 Acc: 0.8616

Epoch 47/99
----------
train Loss: 0.1430 Acc: 0.9468
validation Loss: 0.3427 Acc: 0.8608

Epoch 48/99
----------
train Loss: 0.1470 Acc: 0.9445
validation Loss: 0.3476 Acc: 0.8714

Epoch 49/99
----------
train Loss: 0.1440 Acc: 0.9455
validation Loss: 0.3234 Acc: 0.8665

Epoch 50/99
----------
train Loss: 0.1427 Acc: 0.9495
validation Loss: 0.3568 Acc: 0.8600

Epoch 51/99
----------
train Loss: 0.1492 Acc: 0.9431
validation Loss: 0.3398 Acc: 0.8616

Epoch 52/99
----------
train Loss: 0.1499 Acc: 0.9439
validation Loss: 0.3322 Acc: 0.8657

Epoch 53/99
----------
train Loss: 0.1421 Acc: 0.9475
validation Loss: 0.3604 Acc: 0.8567

Epoch 54/99
----------
train Loss: 0.1461 Acc: 0.9458
validation Loss: 0.3483 Acc: 0.8657

Epoch 55/99
----------
train Loss: 0.1445 Acc: 0.9464
validation Loss: 0.3509 Acc: 0.8591

Epoch 56/99
----------
train Loss: 0.1513 Acc: 0.9436
validation Loss: 0.3411 Acc: 0.8591

Epoch 57/99
----------
train Loss: 0.1433 Acc: 0.9426
validation Loss: 0.3676 Acc: 0.8567

Epoch 58/99
----------
train Loss: 0.1473 Acc: 0.9451
validation Loss: 0.3343 Acc: 0.8714

Epoch 59/99
----------
train Loss: 0.1488 Acc: 0.9460
validation Loss: 0.3519 Acc: 0.8739

Epoch 60/99
----------
train Loss: 0.1500 Acc: 0.9460
validation Loss: 0.3451 Acc: 0.8649

Epoch 61/99
----------
train Loss: 0.1462 Acc: 0.9449
validation Loss: 0.3276 Acc: 0.8649

Epoch 62/99
----------
train Loss: 0.1463 Acc: 0.9472
validation Loss: 0.3255 Acc: 0.8690

Epoch 63/99
----------
train Loss: 0.1489 Acc: 0.9446
validation Loss: 0.3255 Acc: 0.8714

Epoch 64/99
----------
train Loss: 0.1491 Acc: 0.9427
validation Loss: 0.3393 Acc: 0.8616

Epoch 65/99
----------
train Loss: 0.1487 Acc: 0.9456
validation Loss: 0.3551 Acc: 0.8591

Epoch 66/99
----------
train Loss: 0.1470 Acc: 0.9445
validation Loss: 0.3281 Acc: 0.8714

Epoch 67/99
----------
train Loss: 0.1430 Acc: 0.9469
validation Loss: 0.3272 Acc: 0.8649

Epoch 68/99
----------
train Loss: 0.1465 Acc: 0.9454
validation Loss: 0.3492 Acc: 0.8518

Epoch 69/99
----------
train Loss: 0.1484 Acc: 0.9443
validation Loss: 0.3560 Acc: 0.8649

Epoch 70/99
----------
train Loss: 0.1453 Acc: 0.9444
validation Loss: 0.3341 Acc: 0.8722

Epoch 71/99
----------
train Loss: 0.1453 Acc: 0.9475
validation Loss: 0.3438 Acc: 0.8616

Epoch 72/99
----------
train Loss: 0.1442 Acc: 0.9457
validation Loss: 0.3418 Acc: 0.8559

Epoch 73/99
----------
train Loss: 0.1514 Acc: 0.9447
validation Loss: 0.3496 Acc: 0.8575

Epoch 74/99
----------
train Loss: 0.1467 Acc: 0.9452
validation Loss: 0.3292 Acc: 0.8690

Epoch 75/99
----------
train Loss: 0.1460 Acc: 0.9434
validation Loss: 0.3370 Acc: 0.8608

Epoch 76/99
----------
train Loss: 0.1475 Acc: 0.9448
validation Loss: 0.3333 Acc: 0.8739

Epoch 77/99
----------
train Loss: 0.1498 Acc: 0.9450
validation Loss: 0.3376 Acc: 0.8673

Epoch 78/99
----------
train Loss: 0.1424 Acc: 0.9482
validation Loss: 0.3521 Acc: 0.8649

Epoch 79/99
----------
train Loss: 0.1432 Acc: 0.9469
validation Loss: 0.3431 Acc: 0.8649

Epoch 80/99
----------
train Loss: 0.1488 Acc: 0.9442
validation Loss: 0.3252 Acc: 0.8731

Epoch 81/99
----------
train Loss: 0.1472 Acc: 0.9443
validation Loss: 0.3327 Acc: 0.8640

Epoch 82/99
----------
train Loss: 0.1450 Acc: 0.9464
validation Loss: 0.3418 Acc: 0.8681

Epoch 83/99
----------
train Loss: 0.1437 Acc: 0.9461
validation Loss: 0.3408 Acc: 0.8624

Epoch 84/99
----------
train Loss: 0.1473 Acc: 0.9443
validation Loss: 0.3301 Acc: 0.8608

Epoch 85/99
----------
train Loss: 0.1462 Acc: 0.9421
validation Loss: 0.3370 Acc: 0.8632

Epoch 86/99
----------
train Loss: 0.1423 Acc: 0.9480
validation Loss: 0.3502 Acc: 0.8583

Epoch 87/99
----------
train Loss: 0.1505 Acc: 0.9461
validation Loss: 0.3409 Acc: 0.8731

Epoch 88/99
----------
train Loss: 0.1413 Acc: 0.9485
validation Loss: 0.3347 Acc: 0.8632

Epoch 89/99
----------
train Loss: 0.1458 Acc: 0.9460
validation Loss: 0.3512 Acc: 0.8624

Epoch 90/99
----------
train Loss: 0.1486 Acc: 0.9450
validation Loss: 0.3520 Acc: 0.8649

Epoch 91/99
----------
train Loss: 0.1435 Acc: 0.9445
validation Loss: 0.3374 Acc: 0.8673

Epoch 92/99
----------
train Loss: 0.1473 Acc: 0.9456
validation Loss: 0.3596 Acc: 0.8591

Epoch 93/99
----------
train Loss: 0.1435 Acc: 0.9465
validation Loss: 0.3307 Acc: 0.8624

Epoch 94/99
----------
train Loss: 0.1426 Acc: 0.9447
validation Loss: 0.3171 Acc: 0.8714

Epoch 95/99
----------
train Loss: 0.1497 Acc: 0.9454
validation Loss: 0.3368 Acc: 0.8665

Epoch 96/99
----------
train Loss: 0.1436 Acc: 0.9475
validation Loss: 0.3280 Acc: 0.8714

Epoch 97/99
----------
train Loss: 0.1449 Acc: 0.9461
validation Loss: 0.3400 Acc: 0.8608

Epoch 98/99
----------
train Loss: 0.1468 Acc: 0.9454
validation Loss: 0.3587 Acc: 0.8624

Epoch 99/99
----------
train Loss: 0.1428 Acc: 0.9484
validation Loss: 0.3345 Acc: 0.8690

Training complete in 888m 17s
Best validation Acc: 0.875512
test loss: 0.3241592039591362, test acc: 0.8656838656838658
